{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "lmPy-nw7Ww8n"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "#import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I7hafZRZWw8p",
        "outputId": "460014f4-3bbd-4b61-d253-bba5160c4b3c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "PwQUTM_EWw8p"
      },
      "outputs": [],
      "source": [
        "def one_hot_encoder(sequence):\n",
        "    # Create a mapping from nucleotide to a column index\n",
        "    mapping = {'A': 0, 'T': 1, 'C': 2, 'G': 3}\n",
        "\n",
        "    # Initialize a zero matrix with shape (5, length of sequence)\n",
        "    encoded_matrix = np.zeros((4, len(sequence)), dtype=int)\n",
        "\n",
        "    # Populate the matrix based on the sequence\n",
        "    for idx, char in enumerate(sequence.upper()):\n",
        "        if char in mapping:\n",
        "            encoded_matrix[mapping[char], idx] = 1\n",
        "\n",
        "    return encoded_matrix\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "383vq20zWw8q"
      },
      "outputs": [],
      "source": [
        "def read_sequences_from_file(filename):\n",
        "    try:\n",
        "        with open(filename, 'r') as file:\n",
        "            sequences = [(int(line.split(' ', 1)[0]), one_hot_encoder(line.split(' ', 1)[1].strip()))\n",
        "                         for line in file if len(line.split(' ', 1)) == 2]\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: The file {filename} was not found.\")\n",
        "        sequences = []\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        sequences = []\n",
        "    return sequences\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "D0DLO2uDWw8q"
      },
      "outputs": [],
      "source": [
        "class data_class(Dataset):\n",
        "    # def __init__(self,data1):\n",
        "    #     self.data=[]\n",
        "    #     self.labels=[]\n",
        "\n",
        "\n",
        "    #     for line in data1:\n",
        "    #       print('here',line[0])\n",
        "    #       self.data.append(line[1])\n",
        "    #       self.labels.append(line[0])\n",
        "    #       #print(self.data)\n",
        "    #       #print(self.labels)\n",
        "    #     self.data= torch.stack([torch.tensor(d).float() for d in self.data])\n",
        "    #     self.labels=torch.stack([torch.tensor(l).long() for l in self.labels])\n",
        "    #     return self.data, self.labels\n",
        "\n",
        "\n",
        "    def __init__(self,data,label):\n",
        "        self.data = torch.stack([torch.tensor(d).float() for d in data])\n",
        "        self.label = torch.stack([torch.tensor(l).long() for l in label])\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def  shape(self):\n",
        "      return self.data.shape\n",
        "\n",
        "\n",
        "    def __getitem__(self,id):\n",
        "        data_set=self.data[id]\n",
        "        labels=self.label[id]\n",
        "\n",
        "        return data_set,labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "hIFHIR7GWw8r"
      },
      "outputs": [],
      "source": [
        "# Example usage assuming 'sampled.txt' is formatted correctly as \"label sequence\"\n",
        "filename_train = 'Train_sampled.txt'\n",
        "filename_test = 'Test_sampled.txt'\n",
        "\n",
        "sequences_train = read_sequences_from_file(filename_train)\n",
        "sequences_test = read_sequences_from_file(filename_test)\n",
        "data_train = []\n",
        "label_train = []\n",
        "\n",
        "for line in sequences_train:\n",
        "  label_train.append(line[0])\n",
        "  data_train.append(line[1])\n",
        "\n",
        "\n",
        "seq = data_class(data_train,label_train)\n",
        "train_dataloader_1=DataLoader(seq,batch_size=512,shuffle=True)\n",
        "data_test = []\n",
        "label_test = []\n",
        "\n",
        "for line in sequences_test:\n",
        "    label_test.append(line[0])\n",
        "    data_test.append(line[1])\n",
        "\n",
        "# Assuming DataClass has been properly defined and implemented to handle the data\n",
        "seq_test = data_class(data_test, label_test)\n",
        "test_dataloader_1 = DataLoader(seq_test, batch_size=512, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJsVQah58MOt"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrjmeKCW8NuK"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "Yv2Rhi9IWw8q"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ProteinCNN(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, kernel_size=3, stride=1):\n",
        "        super(ProteinCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(in_channels=input_dim, out_channels=64, kernel_size=kernel_size, stride=stride)\n",
        "        self.conv2 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=kernel_size, stride=stride)\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
        "\n",
        "        conv_output_size = self._calculate_conv_output_size(input_dim)\n",
        "        self.fc1 = nn.Linear(conv_output_size, 64)\n",
        "        self.fc2 = nn.Linear(64, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self._forward_conv(x)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "    def _forward_conv(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.pool(x)  \n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.pool(x)  \n",
        "        x = torch.flatten(x, 1)\n",
        "        return x\n",
        "\n",
        "    def _calculate_conv_output_size(self, input_dim):\n",
        "       \n",
        "        with torch.no_grad():\n",
        "            dummy_input = torch.randn(1, input_dim, 199)\n",
        "            conv_output = self._forward_conv(dummy_input)\n",
        "            return conv_output.size(1)\n",
        "\n",
        "input_dim = 4  \n",
        "output_dim = 2 \n",
        "model = ProteinCNN(input_dim, output_dim)\n",
        "\n",
        "optimizer=optim.Adam(params=model.parameters(),lr=0.0001)\n",
        "loss_fn = nn.CrossEntropyLoss()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "__XbJavqWw8s"
      },
      "outputs": [],
      "source": [
        "def train(model,device,train_dataloader,optimizer,epochs):\n",
        "    print(\"inside train\")\n",
        "    model.train()\n",
        "    for batch_ids, (img, classes) in enumerate(train_dataloader):\n",
        "        # print(batch_ids)\n",
        "        # print(img)\n",
        "        # print('class',len(classes))\n",
        "        # print()\n",
        "        # classes=classes.type(torch.LongTensor)\n",
        "        img,classes=img.to(device),classes.to(device)\n",
        "        torch.autograd.set_detect_anomaly(True)\n",
        "        optimizer.zero_grad()\n",
        "        output=model(img)\n",
        "        loss = loss_fn(output,classes)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(\"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
        "        epochs, batch_ids* len(img), len(train_dataloader.dataset),\n",
        "        100.*batch_ids / len(train_dataloader.dataset),loss.item()))\n",
        "\n",
        "def test(model, device, test_dataloader):\n",
        "    model.eval()\n",
        "    test_loss=0\n",
        "    correct=0\n",
        "    with torch.no_grad():\n",
        "        for img,classes in test_dataloader:\n",
        "            img,classes=img.to(device), classes.to(device)\n",
        "            y_hat=model(img)\n",
        "            test_loss+=F.nll_loss(y_hat,classes,reduction='sum').item()\n",
        "            _,y_pred=torch.max(y_hat,1)\n",
        "            correct+=(y_pred==classes).sum().item()\n",
        "        test_loss/=len(test_dataloader)\n",
        "        print(\"\\n Test set: Avarage loss: {:.0f},Accuracy:{}/{} ({:.0f}%)\\n\".format(\n",
        "            test_loss,correct,len(test_dataloader.dataset),100.*correct/len(test_dataloader.dataset)))\n",
        "        print('='*30)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYhmiY1rWw8s",
        "outputId": "4335dd73-369e-419a-ee28-93576469407c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "inside train\n",
            "Train Epoch: 1 [288/800 (0%)]\tLoss: 0.690294\n",
            "\n",
            " Test set: Avarage loss: -8,Accuracy:101/200 (50%)\n",
            "\n",
            "==============================\n",
            "inside train\n",
            "Train Epoch: 2 [288/800 (0%)]\tLoss: 0.686522\n",
            "\n",
            " Test set: Avarage loss: -10,Accuracy:101/200 (50%)\n",
            "\n",
            "==============================\n",
            "inside train\n",
            "Train Epoch: 3 [288/800 (0%)]\tLoss: 0.681335\n",
            "\n",
            " Test set: Avarage loss: -12,Accuracy:108/200 (54%)\n",
            "\n",
            "==============================\n",
            "inside train\n",
            "Train Epoch: 4 [288/800 (0%)]\tLoss: 0.680438\n",
            "\n",
            " Test set: Avarage loss: -14,Accuracy:115/200 (58%)\n",
            "\n",
            "==============================\n",
            "inside train\n",
            "Train Epoch: 5 [288/800 (0%)]\tLoss: 0.679650\n",
            "\n",
            " Test set: Avarage loss: -17,Accuracy:120/200 (60%)\n",
            "\n",
            "==============================\n"
          ]
        }
      ],
      "source": [
        "# WE ARE USING RANDOM DATA SO THE TRAINING AND TESTING DOES NOT MATTER, THE AIM IS TO SHOWCASE THE USE OF A CUSTOM DATASET\n",
        "# SINCE IN PRACTICAL SENSE YOU HAVE TO CLEAN THE DATA AND LOAD THE DATA INTO THE MODEL.\n",
        "\n",
        "\n",
        "if __name__=='__main__':\n",
        "    seed=42\n",
        "    EPOCHS=5\n",
        "\n",
        "    for epoch in range(1,EPOCHS+1):\n",
        "        train(model,device,train_dataloader_1,optimizer,epoch)\n",
        "        test(model,device,test_dataloader_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jcnQ-LWMWw8r",
        "outputId": "3b3fa520-92aa-4674-e212-92d6a7a02efe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "float64\n",
            "float64\n"
          ]
        }
      ],
      "source": [
        "# # Generating random data\n",
        "\n",
        "# random_train_data = np.random.rand(32,1,28, 28)\n",
        "# print(random_train_data.dtype)\n",
        "# random_test_data = np.random.rand(16,1,28, 28)\n",
        "# print(random_test_data.dtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a60wGpCbWw8r"
      },
      "outputs": [],
      "source": [
        "# # Converting the data to tensor type and floating point type\n",
        "\n",
        "# tensor_train_data = torch.from_numpy(random_train_data).float()\n",
        "# tensor_test_data = torch.from_numpy(random_test_data).float()\n",
        "\n",
        "# print(tensor_train_data.shape)\n",
        "# print(tensor_test_data.dtype)\n",
        "# print(len(tensor_data))\n",
        "\n",
        "# # Creating random binary labels. and converting it to tensor\n",
        "\n",
        "# label_test = np.random.choice([0, 1], size=len(tensor_test_data))\n",
        "# label_train = np.random.choice([0, 1], size=len(tensor_train_data))\n",
        "\n",
        "# print(label_train.dtype)\n",
        "# label_test = torch.from_numpy(label_test)\n",
        "# label_train = torch.from_numpy(label_train)\n",
        "# print(label_test.dtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O9PAi3Z-Ww8r"
      },
      "outputs": [],
      "source": [
        "\n",
        "# The most important class, a custom data loader, understand how it is working.\n",
        "\n",
        "# class data_class(Dataset):\n",
        "#     def __init__(self,data,label):\n",
        "#         self.data=data\n",
        "#         self.labels=torch.tensor(label)\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.data)\n",
        "\n",
        "#     def __getitem__(self,id):\n",
        "#         data_set=self.data[id]\n",
        "#         labels=self.labels[id]\n",
        "\n",
        "#         return data_set,labels"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
